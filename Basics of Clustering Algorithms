# Basics of Cluster Algorithms

When presented with large amounts of data for analysis, it becomes increasingly complex to be able to determine how to analyze large scatters of data. The process of testing every single data point as valid to a group will prove to be tedious and time-consuming, especially when there are hundreds of data points to verify. Thankfully, through the use of unsupervised learning[https://towardsdatascience.com/unsupervised-learning-and-data-clustering-eeecb78b422a], clustering algorithms are utilized to speed up the organization in the chaos. Taking a process that could have taken hundreds of man-hours and reducing it to short computing time. Clustering is the act of gathering data points and assorting these points into sectors based on similarities. Computationally, these similarities are described by varying degrees of proximity calculated through models such as Euclidean distances. The algorithm processes the proximities and builds a structure from the determined similarities. Depending on the structure of the cluster, it may be optimal to choose one type of cluster algorithm [https://www.kdnuggets.com/2019/10/right-clustering-algorithm.html#:~:text=The%20centers%20of%20clusters%20should,the%20dataset%20and%20every%20cluster.] over another; doing this may reduce the processing time or increase the accuracy. To increase understanding of these organization types we will cover the basic types of cluster algorithms.

## Basic types of Clustering Algorithms

### Centroid based

This type of algorithm identifies the clusters of data according to the closeness of the data points. The algorithm determines the optimum local point of these cluster points and does an iterative method of calculations for processing. Something note of this note is this requires prior knowledge of the number of clusters in the data set. For example, in the infographic below it would be stated there are three known sets of clusters in the data set and separate these accordingly.
![](https://drive.google.com/file/d/1rjfbLUzpBhtL-G_Eeb3gxtmAfbhdvvIh/view?usp=sharing)
One of the best examples of this algorithm is known as K-means. Where “k” is defined as the total number of determined center points. K-means is a widely utilized resource due to its easy implementation, relatively quick processing speed, scalability, consistent converges, and adaptability to new data sets. Although K-mean offers simplicity it does not handle outliers well, requires a manual k value, produces varying results from the same information, and does not easily identify clusters of varying density and dimensions.  K-means may be used in areas where there is linear information, such as determining test scores or the probability of having heart attacks.

### Connectivity based

Like the connectivity algorithm, this model is based on the Euclidean distance of data points. It classifies the data points close in proximity have more similarity than data points that are wildly spaced apart. Connectivity can be broken down into 2 different approaches: top-down and bottom-up.

The bottom-up method treats every data point as a single cluster and merges each cluster from the increasing distance till all the points are contained within a single cluster; much like mitosis performed in reverse. Top-down approaches use the same method in reverse, where all data points start as a single cluster and divided into independent clusters varying on the subjective distance of the points. These algometric methods are typically visualized through a hierarchical dendrogram chart much like the infographic below.  
![](https://drive.google.com/file/d/1BoTMCw4htlMt6U27tp7MmcluK4hZRiDe/view?usp=sharing)
One of the main examples of this algorithm is known as hierarchal clustering. Unlike K-means, hierarchical does not require prior knowledge of the total number of cluster points and functions quadratically rather than a linear. Given that it has increased complexity it also requires more processing power and offers low efficiency. That being said, it will have reproducible results, can work in multiple dimensions, is not sensitive to the distance metric, and has the unique feature of being able to recover parts of the hierarchy. This type of algorithm can be useful areas such as identifying different classes of plants and animals through similarities in DNA, predicting the stock market, or even determining different classes of cells.
